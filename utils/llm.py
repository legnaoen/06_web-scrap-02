import requests
import logging

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# LM Studio ë¡œì»¬ ì„œë²„ ì£¼ì†Œ (prompt ê¸°ë°˜ ëª¨ë¸ì€ completions ì—”ë“œí¬ì¸íŠ¸ ì‚¬ìš©)
LM_STUDIO_URL = "http://localhost:1234/v1/completions"
def call_llm(text: str, task: str = "translate", model: str = "gemma-3-4b-it-qat") -> str:
    """
    LM Studioì— ìš”ì²­ì„ ë³´ë‚´ ë²ˆì—­ ë˜ëŠ” ìš”ì•½ ê²°ê³¼ë¥¼ ë°˜í™˜ë°›ìŒ

    :param text: ì…ë ¥ í…ìŠ¤íŠ¸
    :param task: 'translate' ë˜ëŠ” 'summarize'
    :param model: ì‚¬ìš©í•  ë¡œì»¬ ëª¨ë¸ ì´ë¦„ (prompt-only ê¸°ë°˜)
    :return: ê²°ê³¼ í…ìŠ¤íŠ¸
    """
    if task == "translate":
        prompt = f"Translate into Korean (no explanation, no repetition, only translated result):\n\n{text}"
    elif task == "summarize":
        prompt = f"Summarize in Korean (no explanation, no repetition):\n\n{text}"
    else:
        raise ValueError("ì§€ì›ë˜ì§€ ì•ŠëŠ” ì‘ì—… ìœ í˜•ì…ë‹ˆë‹¤: 'translate' ë˜ëŠ” 'summarize' ì¤‘ ì„ íƒí•´ì£¼ì„¸ìš”.")

    payload = {
        "model": model,
        "prompt": prompt,
        "max_tokens": 2048,
        "temperature": 0.7
    }

    try:
        logger.info(f"[llm.py] ğŸš€ LLM ìš”ì²­ ì‹œì‘ (task: {task}, model: {model})")
        response = requests.post(LM_STUDIO_URL, json=payload, timeout=60)
        response.raise_for_status()
        data = response.json()
        content = data.get("choices", [{}])[0].get("text", "").strip()
        logger.info(f"[llm.py] âœ… LLM ì‘ë‹µ ìˆ˜ì‹  ì™„ë£Œ (ê¸¸ì´: {len(content)}ì)")
        return content
    except Exception as e:
        logger.error(f"[llm.py] âŒ LLM í˜¸ì¶œ ì‹¤íŒ¨: {e}")
        return "âŒ LLM í˜¸ì¶œ ì‹¤íŒ¨"